{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\34616\\miniconda3\\envs\\gpu_pluja\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Cargar el modelo y tokenizador localmente\n",
    "def load_local_model():\n",
    "    try:\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Detectar si CUDA está disponible\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Intentar cargar el modelo y moverlo a la GPU si está disponible\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except ImportError as e:\n",
    "        st.error(f\"Error importing required backend: {e}\")\n",
    "        st.stop()\n",
    "\n",
    "# Cargar el modelo local\n",
    "model, tokenizer = load_local_model()\n",
    "\n",
    "# Mapeo de etiquetas\n",
    "label_mapping = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# Preprocesar texto\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Función para obtener los scores de cada etiqueta (Negative, Neutral, Positive)\n",
    "def get_sentiment_scores(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)  # Aplicar softmax para obtener las probabilidades\n",
    "    return scores\n",
    "\n",
    "# Función para analizar los sentimientos de un archivo CSV y actualizar la barra de progreso\n",
    "def analyze_sentiments_csv(df):\n",
    "    total_chunks = len(df)\n",
    "    progress_bar = st.progress(0)\n",
    "    progress_text = st.empty()\n",
    "\n",
    "    sentiments = []\n",
    "    negative_scores = []\n",
    "    neutral_scores = []\n",
    "    positive_scores = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        # Obtener los scores de cada sentimiento\n",
    "        try:\n",
    "            scores = get_sentiment_scores(text)\n",
    "            sentiments.append(label_mapping[np.argmax(scores)])  # El sentimiento con mayor puntuación\n",
    "            negative_scores.append(scores[0])\n",
    "            neutral_scores.append(scores[1])\n",
    "            positive_scores.append(scores[2])\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error during sentiment analysis: {e}\")\n",
    "            sentiments.append(\"error\")\n",
    "            negative_scores.append(0)\n",
    "            neutral_scores.append(0)\n",
    "            positive_scores.append(0)\n",
    "\n",
    "        # Actualizar barra de progreso\n",
    "        progress_percentage = (idx + 1) / total_chunks\n",
    "        progress_bar.progress(progress_percentage)\n",
    "        progress_text.text(f\"Processing {idx + 1} of {total_chunks}\")\n",
    "\n",
    "    df['sentiment'] = sentiments\n",
    "    df['negative_score'] = negative_scores\n",
    "    df['neutral_score'] = neutral_scores\n",
    "    df['positive_score'] = positive_scores\n",
    "\n",
    "    # Completar la barra de progreso\n",
    "    progress_bar.progress(1.0)\n",
    "    st.success(\"Sentiment analysis complete!\")\n",
    "\n",
    "    # Convertir el DataFrame en CSV y permitir la descarga\n",
    "    csv = df.to_csv(index=False).encode('utf-8')\n",
    "    st.download_button(\n",
    "        label=\":arrow_down: Download results as CSV\",\n",
    "        data=csv,\n",
    "        file_name='sentiment_analysis_results.csv',\n",
    "        mime='text/csv',\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Función para calcular los porcentajes de cada sentimiento\n",
    "def calculate_sentiment_percentages(df):\n",
    "    sentiment_counts = df['sentiment'].value_counts(normalize=True) * 100\n",
    "    return [sentiment_counts.get('Negative', 0), sentiment_counts.get('Neutral', 0), sentiment_counts.get('Positive', 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_pluja",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
