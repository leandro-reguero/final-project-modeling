{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "from transformers import pipeline\n",
                "from transformers import RobertaTokenizer\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "import gc\n",
                "\n",
                "import os\n",
                "\n",
                "from huggingface_hub import InferenceClient\n",
                "from transformers import AutoTokenizer\n",
                "import logging\n",
                "import time\n",
                "import requests\n",
                "from tqdm import tqdm\n",
                "import warnings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Index(['Unnamed: 0', 'text', 'submission_type', 'subreddit', 'label'], dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "# Cargar el DataFrame con los comentarios de Reddit\n",
                "data = pd.read_csv(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_clean.csv\")\n",
                "\n",
                "# Verifica que las columnas necesarias estén en el DataFrame\n",
                "print(data.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Autenticación con tu token de Hugging Face\n",
                "client = InferenceClient(token=\"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\")\n",
                "\n",
                "# Cargar el tokenizador\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
                "\n",
                "# Función para dividir el texto en fragmentos, respetando el límite de tokens\n",
                "def chunk_text(text, tokenizer, chunk_size=512):\n",
                "    tokens = tokenizer(text, truncation=True, max_length=chunk_size, return_tensors='pt')\n",
                "    input_ids = tokens.input_ids[0]\n",
                "    for i in range(0, len(input_ids), chunk_size):\n",
                "        chunk_ids = input_ids[i:i + chunk_size]\n",
                "        yield tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
                "\n",
                "\n",
                "# Iniciando variables de backoff:\n",
                "initial_backoff = 5\n",
                "max_backoff = 300\n",
                "backoff_factor = 2\n",
                "def backoff_sleep(intento):\n",
                "    sleep_time = min(initial_backoff * (backoff_factor ** intento), max_backoff)\n",
                "    logging.info(f\"Rate limit hit. Sleeping for {sleep_time} seconds...\")\n",
                "    time.sleep(sleep_time)\n",
                "\n",
                "ruta_guardado = pd.read_csv(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_clean.csv\")\n",
                "\n",
                "def guardar_progreso(df):\n",
                "    if not df.empty:\n",
                "        try:\n",
                "            logging.info(\"Guardando el progreso...\")\n",
                "            arch_existe = os.path.isfile(ruta_guardado)\n",
                "            # si el archivo existe, guardamos\n",
                "            df.to_csv(ruta_guardado, mode='a', header = not arch_existe, index=False)\n",
                "            logging.info(f\"Se han guardado {len(df)} instancias correctamente\")\n",
                "        \n",
                "        except Exception as e:\n",
                "            logging.error(f\"Ha habido un error al guardar el progreso: {e}\")\n",
                "            logging.info(f\"{len(df)} instancias no guardadas\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_clean.csv\")\n",
                "save_threshold = 1000\n",
                "records_collected = 0\n",
                "sentiment_list = []\n",
                "for i in range(0, len(df), save_threshold):\n",
                "    print(i)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(187675, 5)"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Función para realizar el análisis de sentimientos en fragmentos\n",
                "columns = ['text', 'submission_type', 'subreddit', 'label', 'sentiment']\n",
                "df = pd.read_csv(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_clean.csv\")\n",
                "\n",
                "\n",
                "def analyze_sentiments_chunked(texts, tokenizer, chunk_size=512, save_threshold = 10000, process_chunk_size = 5000):\n",
                "    intento = 0\n",
                "    sentiment_list = []\n",
                "    num_procesados = 0\n",
                "\n",
                "    for start in range(0, len(df), process_chunk_size):\n",
                "        end = min(start + process_chunk_size, len(df))\n",
                "        chunk_df = df.iloc[start:end]\n",
                "        chunk_sentiments = []\n",
                "        \n",
                "        for idx, text in enumerate(chunk_df['text']):\n",
                "            while True:\n",
                "                try: \n",
                "                    chunks = list(chunk_text(text, tokenizer, chunk_size=chunk_size))\n",
                "                    break # salir del loop si va guay\n",
                "\n",
                "                except requests.exceptions.HTTPError as e:\n",
                "                    if e.response.status_code == 429:\n",
                "                        backoff_sleep(intento)\n",
                "                        intento +=1\n",
                "                    else:\n",
                "                        logging.info('Oops, problema desconocido lol')\n",
                "                        break\n",
                "\n",
                "            for chunk in chunks:\n",
                "                response = client.text_classification(\n",
                "                model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
                "                text=chunk\n",
                "                )\n",
                "\n",
                "\n",
                "            chunk_sentiments.append(response)\n",
                "        \n",
                "        # asignamos sentiment al chunk\n",
                "        chunk_df['sentiment'] = chunk_sentiments\n",
                "\n",
                "        # guardamos los sentiments en una lista grande\n",
                "        sentiment_list.extend(chunk_sentiments)\n",
                "            \n",
                "                \n",
                "\n",
                "\n",
                "        \n",
                "\n",
                "\n",
                "            \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "# haciendo logging para monitorear errores de rate-limit y retries\n",
                "logging.basicConfig(level=logging.INFO, \n",
                "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
                "                    handlers = [\n",
                "                        logging.StreamHandler(), \n",
                "                        logging.FileHandler(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\modelado\\bert_log_v1.log\", \n",
                "                                            mode= 'a')\n",
                "                                ]\n",
                "                    )\n",
                "\n",
                "\n",
                "logging.info(\"hola hola probandno 123\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "# Set up logging to print messages to the console\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO, \n",
                "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
                "    handlers=[logging.StreamHandler()]\n",
                ")\n",
                "\n",
                "logging.info(f\"Hola esto es un test\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\34616\\miniconda3\\envs\\GPU_Chiclanera\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "import logging\n",
                "import os\n",
                "import time\n",
                "import pandas as pd\n",
                "from huggingface_hub import InferenceClient\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# haciendo logging para monitorear errores de rate-limit y retries\n",
                "logging.basicConfig(level=logging.INFO, \n",
                "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
                "                    handlers = [\n",
                "                        logging.StreamHandler(), \n",
                "                        logging.FileHandler(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\modelado\\bert_log_v1.log\", \n",
                "                                            mode= 'a')\n",
                "                                ]\n",
                "                    )\n",
                "\n",
                "# Autenticación con tu token de Hugging Face\n",
                "client = InferenceClient(token=\"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\")\n",
                "\n",
                "# Cargar el tokenizador\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
                "\n",
                "# Function to split text into chunks respecting the token limit\n",
                "def chunk_text(text, tokenizer, chunk_size=512):\n",
                "    tokens = tokenizer(text, truncation=True, max_length=chunk_size, return_tensors='pt')\n",
                "    input_ids = tokens.input_ids[0]\n",
                "    for i in range(0, len(input_ids), chunk_size):\n",
                "        chunk_ids = input_ids[i:i + chunk_size]\n",
                "        yield tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
                "\n",
                "# Backoff variables\n",
                "initial_backoff = 5\n",
                "max_backoff = 300\n",
                "backoff_factor = 2\n",
                "\n",
                "def backoff_sleep(intento):\n",
                "    sleep_time = min(initial_backoff * (backoff_factor ** intento), max_backoff)\n",
                "    logging.info(f\"Rate limit hit. Sleeping for {sleep_time} seconds...\")\n",
                "    time.sleep(sleep_time)\n",
                "\n",
                "# Define save path\n",
                "ruta_guardado = r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_clean.csv\"\n",
                "\n",
                "# Function to save progress\n",
                "def guardar_progreso(df):\n",
                "    if not df.empty:\n",
                "        try:\n",
                "            logging.info(\"Guardando el progreso...\")\n",
                "            arch_existe = os.path.isfile(ruta_guardado)\n",
                "            # If the file exists, append without header; otherwise, write with header\n",
                "            df.to_csv(ruta_guardado, mode='a', header=not arch_existe, index=False)\n",
                "            logging.info(f\"Se han guardado {len(df)} instancias correctamente\")\n",
                "        \n",
                "        except Exception as e:\n",
                "            logging.error(f\"Ha habido un error al guardar el progreso: {e}\")\n",
                "            logging.info(f\"{len(df)} instancias no guardadas\")\n",
                "\n",
                "# Main function to analyze sentiments and save progress\n",
                "def analyze_sentiments_chunked(df, tokenizer, rate_limit_sleep, chunk_size=512, save_threshold=10000, process_chunk_size=1000):\n",
                "    intento = 0\n",
                "    processed_count = 0\n",
                "    ch_num = 0\n",
                "    # Process the dataframe in chunks of `process_chunk_size`\n",
                "    for start in range(0, len(df), process_chunk_size):\n",
                "        ch_num +=1        \n",
                "        end = min(start + process_chunk_size, len(df))\n",
                "        chunk_df = df.iloc[start:end]\n",
                "        sentiment_list = []\n",
                "        logging.info(f\"Analyzing chunk n.{ch_num}\")\n",
                "        print(f\"Procesando chunk-group n.{ch_num}\")\n",
                "        for idx, text in enumerate(chunk_df['text']):\n",
                "            while True:\n",
                "                try: \n",
                "                    chunks = list(chunk_text(text, tokenizer, chunk_size=chunk_size))\n",
                "                    break # Exit the loop if successful\n",
                "\n",
                "                except requests.exceptions.HTTPError as e:\n",
                "                    if e.response.status_code == 429:\n",
                "                        backoff_sleep(intento)\n",
                "                        intento += 1\n",
                "                    else:\n",
                "                        logging.info('Oops, problema desconocido lol')\n",
                "                        break\n",
                "\n",
                "            # Analyze sentiment for each chunk\n",
                "            overall_sentiment = None\n",
                "            max_score = -1  # Initialize to a value that any score will surpass\n",
                "\n",
                "            for chunk in chunks:\n",
                "                while True:\n",
                "                    try:\n",
                "                        response = client.text_classification(\n",
                "                            model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
                "                            text=chunk\n",
                "                        )\n",
                "                        break\n",
                "                    except requests.exceptions.HTTPError as e:\n",
                "                        if e.response.status_code == 429:\n",
                "                            backoff_sleep(intento)\n",
                "                            intento +=1\n",
                "                        else:\n",
                "                            logging.info(f'Ha habido un problema inesperado: {e}')\n",
                "                            break\n",
                "\n",
                "                # Find the label with the highest score\n",
                "                for element in response:\n",
                "                    if element.score > max_score:\n",
                "                        max_score = element.score\n",
                "                        overall_sentiment = element.label\n",
                "\n",
                "            sentiment_list.append(overall_sentiment)\n",
                "\n",
                "            # sleep entre requests para evitar llegar al límite\n",
                "            time.sleep(rate_limit_sleep)\n",
                "\n",
                "\n",
                "        # Assign sentiments to the chunk\n",
                "        df.loc[start:end-1, 'sentiment'] = sentiment_list\n",
                "        \n",
                "        # Increment the processed count\n",
                "        processed_count += len(chunk_df)\n",
                "\n",
                "        # Save progress after every `save_threshold` rows\n",
                "        if processed_count >= save_threshold:\n",
                "            guardar_progreso(df.iloc[start:end])\n",
                "            processed_count = 0\n",
                "        \n",
                "\n",
                "\n",
                "    # Final save for any remaining data\n",
                "    guardar_progreso(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Procesando chunk-group n.1\n"
                    ]
                }
            ],
            "source": [
                "df = pd.read_csv(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_clean.csv\")\n",
                "df = df[:5000]\n",
                "analyze_sentiments_chunked(df, tokenizer, rate_limit_sleep = 0.5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>submission_type</th>\n",
                            "      <th>subreddit</th>\n",
                            "      <th>label</th>\n",
                            "      <th>sentiment</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Media: Nobody knows what kamala is about\\n\\nMe...</td>\n",
                            "      <td>comment</td>\n",
                            "      <td>politics</td>\n",
                            "      <td>kamala</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>NYT breaking news that Netanyahu has agreed to...</td>\n",
                            "      <td>comment</td>\n",
                            "      <td>politics</td>\n",
                            "      <td>trumper</td>\n",
                            "      <td>LABEL_2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>I love how the stock crash 2 weeks ago was HUG...</td>\n",
                            "      <td>comment</td>\n",
                            "      <td>politics</td>\n",
                            "      <td>trump</td>\n",
                            "      <td>LABEL_2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>I was thinking this morning about how freaking...</td>\n",
                            "      <td>comment</td>\n",
                            "      <td>politics</td>\n",
                            "      <td>trumper</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Conservative in a purple state. I'm voting for...</td>\n",
                            "      <td>comment</td>\n",
                            "      <td>politics</td>\n",
                            "      <td>trump</td>\n",
                            "      <td>LABEL_0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                text submission_type  \\\n",
                            "0  Media: Nobody knows what kamala is about\\n\\nMe...         comment   \n",
                            "1  NYT breaking news that Netanyahu has agreed to...         comment   \n",
                            "2  I love how the stock crash 2 weeks ago was HUG...         comment   \n",
                            "3  I was thinking this morning about how freaking...         comment   \n",
                            "4  Conservative in a purple state. I'm voting for...         comment   \n",
                            "\n",
                            "  subreddit    label sentiment  \n",
                            "0  politics   kamala   LABEL_1  \n",
                            "1  politics  trumper   LABEL_2  \n",
                            "2  politics    trump   LABEL_2  \n",
                            "3  politics  trumper   LABEL_1  \n",
                            "4  politics    trump   LABEL_0  "
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# cuando lo de arriba termine:\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(5000, 5)"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.shape"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.19"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
